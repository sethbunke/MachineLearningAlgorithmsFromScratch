{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\"\"\" First change the following directory link to where all input files do exist \"\"\"\n",
    "os.chdir(\"D:\\\\Book writing\\\\Codes\\\\Chapter 5\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# KNN Curse of Dimensionality\n",
    "import random,math\n",
    "\n",
    "def random_point_gen(dimension):\n",
    "    return [random.random() for _ in range(dimension)]\n",
    "\n",
    "def distance(v,w):\n",
    "    vec_sub = [v_i-w_i for v_i,w_i in zip(v,w)]\n",
    "    sum_of_sqrs = sum(v_i*v_i for v_i in vec_sub)\n",
    "    return math.sqrt(sum_of_sqrs)\n",
    "\n",
    "def random_distances_comparison(dimension,number_pairs):\n",
    "    return [distance(random_point_gen(dimension),random_point_gen(dimension))\n",
    "            for _ in range(number_pairs)]\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "dimensions = range(1, 201, 5)\n",
    "\n",
    "avg_distances = []\n",
    "min_distances = []\n",
    "\n",
    "\n",
    "dummyarray = np.empty((20,4))\n",
    "dist_vals = pd.DataFrame(dummyarray)\n",
    "dist_vals.columns = [\"Dimension\",\"Min_Distance\",\"Avg_Distance\",\"Min/Avg_Distance\"]\n",
    "\n",
    "random.seed(34)\n",
    "i = 0\n",
    "for dims in dimensions:\n",
    "    distances = random_distances_comparison(dims, 1000)  \n",
    "    avg_distances.append(mean(distances))    \n",
    "    min_distances.append(min(distances))     \n",
    "    \n",
    "    dist_vals.loc[i,\"Dimension\"] = dims\n",
    "    dist_vals.loc[i,\"Min_Distance\"] = min(distances)\n",
    "    dist_vals.loc[i,\"Avg_Distance\"] = mean(distances)\n",
    "    dist_vals.loc[i,\"Min/Avg_Distance\"] = min(distances)/mean(distances)\n",
    "                 \n",
    "    print(dims, min(distances), mean(distances), min(distances)*1.0 / mean(distances))\n",
    "    i = i+1\n",
    "\n",
    "# Ploting Average distances for Various Dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "#plt.title('Avg. Distance Change with Number of Dimensions for 1K Obs')\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Avg. Distance')\n",
    "plt.plot(dist_vals[\"Dimension\"],dist_vals[\"Avg_Distance\"])\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 1-Dimension Plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "one_d_data = np.random.rand(60,1)\n",
    "one_d_data_df = pd.DataFrame(one_d_data)\n",
    "one_d_data_df.columns = [\"1D_Data\"]\n",
    "one_d_data_df[\"height\"] = 1\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(one_d_data_df['1D_Data'],one_d_data_df[\"height\"])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"1-D points\")\n",
    "plt.show()\n",
    "\n",
    "# 2- Dimensions Plot\n",
    "two_d_data = np.random.rand(60,2)\n",
    "two_d_data_df = pd.DataFrame(two_d_data)\n",
    "two_d_data_df.columns = [\"x_axis\",\"y_axis\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(two_d_data_df['x_axis'],two_d_data_df[\"y_axis\"])\n",
    "plt.xlabel(\"x_axis\");plt.ylabel(\"y_axis\")\n",
    "plt.show()\n",
    "\n",
    "# 3- Dimensions Plot\n",
    "three_d_data = np.random.rand(60,3)\n",
    "three_d_data_df = pd.DataFrame(three_d_data)\n",
    "three_d_data_df.columns = [\"x_axis\",\"y_axis\",\"z_axis\"]\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(three_d_data_df['x_axis'],three_d_data_df[\"y_axis\"],three_d_data_df[\"z_axis\"])\n",
    "ax.set_xlabel('x_axis')\n",
    "ax.set_ylabel('y_axis')\n",
    "ax.set_zlabel('z_axis')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# KNN CLassifier - Breast Cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "breast_cancer = pd.read_csv(\"Breast_Cancer_Wisconsin.csv\")\n",
    "\n",
    "print (breast_cancer.head())\n",
    "\n",
    "breast_cancer['Bare_Nuclei'] = breast_cancer['Bare_Nuclei'].replace('?', np.NAN)\n",
    "breast_cancer['Bare_Nuclei'] = breast_cancer['Bare_Nuclei'].fillna(breast_cancer['Bare_Nuclei'].value_counts().index[0])\n",
    "\n",
    "breast_cancer['Cancer_Ind'] = 0\n",
    "breast_cancer.loc[breast_cancer['Class']==4,'Cancer_Ind'] = 1\n",
    "\n",
    "x_vars = breast_cancer.drop(['ID_Number','Class','Cancer_Ind'],axis=1)\n",
    "y_var = breast_cancer['Cancer_Ind']\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_vars_stdscle = StandardScaler().fit_transform(x_vars.values)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_vars_stdscle_df = pd.DataFrame(x_vars_stdscle, index=x_vars.index, columns=x_vars.columns)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_vars_stdscle_df,y_var,train_size = 0.7,random_state=42)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_fit = KNeighborsClassifier(n_neighbors=3,p=2,metric='minkowski')\n",
    "knn_fit.fit(x_train,y_train)\n",
    "\n",
    "print (\"\\nK-Nearest Neighbors - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,knn_fit.predict(x_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]) )     \n",
    "print (\"\\nK-Nearest Neighbors - Train accuracy:\",round(accuracy_score(y_train,knn_fit.predict(x_train)),3))\n",
    "print (\"\\nK-Nearest Neighbors - Train Classification Report\\n\",classification_report(y_train,knn_fit.predict(x_train)))\n",
    "\n",
    "print (\"\\n\\nK-Nearest Neighbors - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,knn_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \n",
    "print (\"\\nK-Nearest Neighbors - Test accuracy:\",round(accuracy_score(y_test,knn_fit.predict(x_test)),3))\n",
    "print (\"\\nK-Nearest Neighbors - Test Classification Report\\n\",classification_report(y_test,knn_fit.predict(x_test)))\n",
    "\n",
    "\n",
    "# Tuning of K- value for Train & Test data\n",
    "dummyarray = np.empty((5,3))\n",
    "k_valchart = pd.DataFrame(dummyarray)\n",
    "k_valchart.columns = [\"K_value\",\"Train_acc\",\"Test_acc\"]\n",
    "\n",
    "k_vals = [1,2,3,4,5]\n",
    "for i in range(len(k_vals)):\n",
    "    knn_fit = KNeighborsClassifier(n_neighbors=k_vals[i],p=2,metric='minkowski')\n",
    "    knn_fit.fit(x_train,y_train)\n",
    "\n",
    "    print (\"\\nK-value\",k_vals[i])\n",
    "    \n",
    "    tr_accscore = round(accuracy_score(y_train,knn_fit.predict(x_train)),3)\n",
    "    print (\"\\nK-Nearest Neighbors - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,knn_fit.predict(x_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]) )     \n",
    "    print (\"\\nK-Nearest Neighbors - Train accuracy:\",tr_accscore)\n",
    "    print (\"\\nK-Nearest Neighbors - Train Classification Report\\n\",classification_report(y_train,knn_fit.predict(x_train)))\n",
    "\n",
    "    ts_accscore = round(accuracy_score(y_test,knn_fit.predict(x_test)),3)    \n",
    "    print (\"\\n\\nK-Nearest Neighbors - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,knn_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \n",
    "    print (\"\\nK-Nearest Neighbors - Test accuracy:\",ts_accscore)\n",
    "    print (\"\\nK-Nearest Neighbors - Test Classification Report\\n\",classification_report(y_test,knn_fit.predict(x_test)))\n",
    "    \n",
    "    k_valchart.loc[i, 'K_value'] = k_vals[i]      \n",
    "    k_valchart.loc[i, 'Train_acc'] = tr_accscore     \n",
    "    k_valchart.loc[i, 'Test_acc'] = ts_accscore               \n",
    "\n",
    "\n",
    "# Ploting accuracies over varied K-values\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "#plt.title('KNN Train & Test Accuracy change with K-value')\n",
    "\n",
    "plt.xlabel('K-value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(k_valchart[\"K_value\"],k_valchart[\"Train_acc\"])\n",
    "plt.plot(k_valchart[\"K_value\"],k_valchart[\"Test_acc\"])\n",
    "\n",
    "plt.axis([0.9,5, 0.92, 1.005])\n",
    "plt.xticks([1,2,3,4,5])\n",
    "\n",
    "for a,b in zip(k_valchart[\"K_value\"],k_valchart[\"Train_acc\"]):\n",
    "    plt.text(a, b, str(b),fontsize=10)\n",
    "\n",
    "for a,b in zip(k_valchart[\"K_value\"],k_valchart[\"Test_acc\"]):\n",
    "    plt.text(a, b, str(b),fontsize=10)\n",
    "    \n",
    "plt.legend(loc='upper right')    \n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes using NLP\n",
    "\n",
    "# USe following code if it wont work in first place with UTF-8 code error\n",
    "\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "\n",
    "import csv\n",
    "\n",
    "smsdata = open('SMSSpamCollection.txt','r')\n",
    "csv_reader = csv.reader(smsdata,delimiter='\\t')\n",
    "\n",
    "smsdata_data = []\n",
    "smsdata_labels = []\n",
    "\n",
    "for line in csv_reader:\n",
    "    smsdata_labels.append(line[0])\n",
    "    smsdata_data.append(line[1])\n",
    "\n",
    "smsdata.close()\n",
    "\n",
    "# Printing top 5 lines\n",
    "for i in range(5):\n",
    "    print (smsdata_data[i],smsdata_labels[i])\n",
    "\n",
    "# Printing Spam & Ham count\n",
    "from collections import Counter\n",
    "c = Counter( smsdata_labels )\n",
    "print(c)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocessing(text):\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in\n",
    "              nltk.word_tokenize(sent)]\n",
    "    \n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    tagged_corpus = pos_tag(tokens)    \n",
    "    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    \n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
    "\n",
    "    return pre_proc_text\n",
    "\n",
    "\n",
    "smsdata_data_2 = []\n",
    "\n",
    "for i in smsdata_data:\n",
    "    smsdata_data_2.append(preprocessing(i))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "trainset_size = int(round(len(smsdata_data_2)*0.70))\n",
    "\n",
    "\n",
    "print ('The training set size for this classifier is ' + str(trainset_size) + '\\n')\n",
    "\n",
    "x_train = np.array([''.join(rec) for rec in smsdata_data_2[0:trainset_size]])\n",
    "y_train = np.array([rec for rec in smsdata_labels[0:trainset_size]])\n",
    "x_test = np.array([''.join(rec) for rec in smsdata_data_2[trainset_size+1:len(smsdata_data_2)]])\n",
    "y_test = np.array([rec for rec in smsdata_labels[trainset_size+1:len(smsdata_labels)]])\n",
    "\n",
    "\n",
    "# building TFIDF vectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english', \n",
    "                             max_features= 4000,strip_accents='unicode',  norm='l2')\n",
    "\n",
    "x_train_2 = vectorizer.fit_transform(x_train).todense()\n",
    "x_test_2 = vectorizer.transform(x_test).todense()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(x_train_2, y_train)\n",
    "\n",
    "ytrain_nb_predicted = clf.predict(x_train_2)\n",
    "ytest_nb_predicted = clf.predict(x_test_2)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "print (\"\\nNaive Bayes - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,ytrain_nb_predicted,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \n",
    "print (\"\\nNaive Bayes- Train accuracy\",round(accuracy_score(y_train,ytrain_nb_predicted),3))\n",
    "print (\"\\nNaive Bayes  - Train Classification Report\\n\",classification_report(y_train,ytrain_nb_predicted))\n",
    "\n",
    "print (\"\\nNaive Bayes - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,ytest_nb_predicted,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \n",
    "print (\"\\nNaive Bayes- Test accuracy\",round(accuracy_score(y_test,ytest_nb_predicted),3))\n",
    "print (\"\\nNaive Bayes  - Test Classification Report\\n\",classification_report(y_test,ytest_nb_predicted))\n",
    "\n",
    "\n",
    "# printing top features \n",
    "feature_names = vectorizer.get_feature_names()\n",
    "coefs = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "\n",
    "print (\"\\n\\nTop 10 features - both first & last\\n\")\n",
    "n=10\n",
    "top_n_coefs = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "for (coef_1, fn_1), (coef_2, fn_2) in top_n_coefs:\n",
    "    print('\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s' % (coef_1, fn_1, coef_2, fn_2))\n",
    "\n",
    "\n",
    "© 2018 GitHub, Inc.\n",
    "Terms\n",
    "Privacy\n",
    "Security\n",
    "Status\n",
    "Help\n",
    "Contact GitHub\n",
    "API\n",
    "Training\n",
    "Shop\n",
    "Blog\n",
    "About\n",
    "Press h to open a hovercard with more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow2]",
   "language": "python",
   "name": "Python [tensorflow2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
